{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Neural Network!!</h1>\n",
    "\n",
    "Since I quite already know what the neural network is, if I have a time I will write the definition later. For now, I'll represent the example of neural network implementation on each type of neural network (pytorch & tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Type of neural network that we're going to learn today : </h2>\n",
    "\n",
    "1. Standard Neural Network\n",
    "\n",
    "2. Convolutional Neural Network\n",
    "\n",
    "3. Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Activation Function </h3>\n",
    "  Before we go any further, I would like to introduce the \"Activation function\" that makes some output easier to compute. \n",
    "  \n",
    "  1. Sigmoid Activation Function \n",
    "  \n",
    "  2. Hyperbolic Tangent Activation Function (tanh)\n",
    "  \n",
    "  3. Rectified Linear Unit Activation Function (ReLU)\n",
    "  \n",
    "  4. Leaky ReLU \n",
    "  \n",
    "  5. Exponential Linear Units (ELU)\n",
    "  \n",
    "  6. Maxout\n",
    "  \n",
    "  More detail here : https://medium.com/@shrutijadon10104776/survey-on-activation-functions-for-deep-learning-9689331ba092"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Standard Neural Network : Implementation </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the naive implementation : later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow implementation\n",
    "import tensorflow as tf\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.4180 - accuracy: 0.7440\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.6229 - accuracy: 0.8364\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4867 - accuracy: 0.8689\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4242 - accuracy: 0.8870\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3905 - accuracy: 0.8973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x108239710>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python optimisation variables\n",
    "learning_rate = 0.5\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "## This is for Tensorflow v1 only\n",
    "## Placeholder is a variable that we don't assign any value yet, (like input or output, or any structure)\n",
    "#x = tf.placeholder(tf.float32, [None, 784])\n",
    "#y = tf.placeholder(tf.float32, [None,10])\n",
    "\n",
    "#W1 = tf.Variable(tf.random.normal([784,30], stddev=0.03), name='W1')\n",
    "#b1 = tf.constant(tf.random.normal([30]), name='b1')\n",
    "\n",
    "#W2 = tf.Variable(tf.random.normal([30,10], stddev=0.03), name='W2')\n",
    "#b2 = tf.constant(tf.random.normal([10],name='b2'))\n",
    "\n",
    "#hidden_out = tf.add(tf.matmul(W1,x),b1)\n",
    "#hidden_out = tf.nn.relu(hidden_out)\n",
    "#y_ = tf.nn.softmax(tf.add(tf.matmul(W2,hidden_out),b2)) \n",
    "##### I'm gonna end tensorflow v1 here, since it's deprecated\n",
    "\n",
    "## Tensorflow v2 (and from now on, I'll use tensorflow v2)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = (28,28)), # Flatten the 2D to 1D \n",
    "    tf.keras.layers.Dense(128,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2), # Randomly set input to 0 with a frequency of \"rate\" (in this case : 0.2)\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam',\n",
    "             loss=loss_fn,\n",
    "             metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.3343 - accuracy: 0.9376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3342955410480499, 0.9376000165939331]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test,verbose=2) # Verbose is about how do you want to see on trainning process (Display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start trainning : \n",
      "#0 Loss: 0.43502917885780334\n",
      "#100 Loss: 0.10000000149011612\n",
      "#200 Loss: 0.10000000149011612\n",
      "#300 Loss: 0.10000000149011612\n",
      "#400 Loss: 0.10000000149011612\n",
      "#500 Loss: 0.10000000149011612\n",
      "#600 Loss: 0.10000000149011612\n",
      "#700 Loss: 0.10000000149011612\n",
      "#800 Loss: 0.10000000149011612\n",
      "#900 Loss: 0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "#Pytorch implementation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class Neural_Network(nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(Neural_Network,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.W1 = torch.randn(self.input_size,128)  \n",
    "        self.W2 = torch.randn(128,self.output_size)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = torch.flatten(X,start_dim=1)\n",
    "        self.z = torch.matmul(X,self.W1)\n",
    "        self.z2 = self.sigmoid(self.z)\n",
    "        self.z3 = torch.matmul(self.z2, self.W2)\n",
    "        o = self.sigmoid(self.z3)\n",
    "        return o\n",
    "    \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + torch.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        # derivative of sigmoid\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def backward(self,X,y,o):\n",
    "        X = torch.flatten(X,start_dim=1)\n",
    "        self.o_error = y-o\n",
    "        self.o_delta = self.o_error*self.sigmoidPrime(o)\n",
    "        self.z2_error = torch.matmul(self.o_delta,torch.t(self.W2)) # torch.t(X) is to transpose X\n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.z2)\n",
    "        self.W1 += torch.matmul(torch.t(X),self.z2_delta)\n",
    "        self.W2 += torch.matmul(torch.t(self.z2),self.o_delta)\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        # forward + backward pass for training\n",
    "        X = torch.flatten(X,start_dim=1)\n",
    "        o = self.forward(X)\n",
    "        self.backward(X, y, o)\n",
    "    \n",
    "    def predict(self):\n",
    "        print (\"Predicted data based on trained weights: \")\n",
    "        print (\"Input (scaled): \\n\" + str(xPredicted))\n",
    "        print (\"Output: \\n\" + str(self.forward(xPredicted)))\n",
    "\n",
    "X = torch.from_numpy(x_train).float()\n",
    "y = [[1 if i == x else 0 for i in range(10)] for x in y_train]\n",
    "y = torch.from_numpy(np.array(y)).float()\n",
    "NN = Neural_Network(784,10)\n",
    "print(\"Start trainning : \")\n",
    "for i in range(1000):  # trains the NN 1,000 times\n",
    "    if i % 100 == 0:\n",
    "        print(\"#\" + str(i) + \" Loss: \" + str(torch.mean((y - NN(X))**2).detach().item()))  # mean sum squared loss\n",
    "    NN.train(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note1 : We can use torch.save(model,name) to save all weight in neural network. \n",
    "\n",
    "Note2 :  detach() is the method to detach from the pytorch graph, and that value will not contribute gradient in the main graph. See more explanation at : http://www.bnikolic.co.uk/blog/pytorch-detach.html#:~:text=The%20detach()%20method%20constructs,visualised%20using%20the%20torchviz%20package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#100 Loss: 828414272.0\n",
      "#200 Loss: 235820.078125\n",
      "#300 Loss: 67.22174835205078\n",
      "#400 Loss: 0.10908998548984528\n",
      "#500 Loss: 0.08998025208711624\n",
      "#600 Loss: 0.08997682482004166\n",
      "#700 Loss: 0.08997835218906403\n",
      "#800 Loss: 0.08997835963964462\n",
      "#900 Loss: 0.08997835963964462\n",
      "#1000 Loss: 0.08997835963964462\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Flatten(),\n",
    "                      nn.Linear(784,128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(0.2),\n",
    "                      nn.Linear(128,10))\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.2)\n",
    "optimizer.zero_grad()\n",
    "for i in range(1000):  # trains the NN 1,000 times\n",
    "    loss = loss_fn(model(X),y)\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(\"#\" + str(i+1) + \" Loss: \" + str(loss.item()))  # mean sum squared loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
