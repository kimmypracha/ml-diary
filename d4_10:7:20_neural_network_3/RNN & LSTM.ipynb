{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Recurrent Neural Network </h1>\n",
    "\n",
    "Since the standard Neural Network can't use the previous data for considering an output. It might not be that useful when we have to deal with continuous data like text data. Since each word has some relationship to each other. So that's why we have to come up with something like \"Recurrent Neural Network\"\n",
    "(Image for the RNN architecture will post soon.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the RNN has \"hidden state\" that keep the previous output into consideration to regulate the model. So before we go any further for the neural network regulation. RNN will combine both hidden state and input value and do the same thing like the standard neural network did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Backpropagation Through Time (BPTT) </h2>\n",
    "\n",
    "Like the normal backpropagation, but this time, we are dealing with hidden state. Theorectically, we have to do backpropagation through all previous hidden state. However, it would take a lot of time, so we use approximate BPTT instead. (I'll come back here later.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Exploding & Vanishing gradient </h2>\n",
    "\n",
    "Because of the backpropagation through time, the value can be exploded to infinity or truncated to zero (since computer can not store that much or that less of that number). That would be a problem for RNN, and we cannot continue our model anymore. So we come up with something more intellectual, which is LSTM (Long short term memory.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Long-Short Term Memory (LSTM) </h1>\n",
    "\n",
    "LSTM has something different from RNN, which is Long-Term memory (cell state), it also has a forget gate to help the model forget unnecessary data. Let's look on each gate in LSTM.\n",
    "\n",
    "1. Input Gate : It will combine both input and hidden state (short-term memory from previous output) to the sigmoid function (to decide which data should keep (1) or should delete (0)), and also send it to tanh function to regulate the neural network. Then we multiply them together to filter for the real input it should be.\n",
    "\n",
    "2. Forget Gate : This is a quite important part in LSTM. The forget gate will combine hidden state and input together to the forget vector and compute them to the sigmoid function (0 forget, 1 remember) and we will multiply the result with cell state (long-term memory) to choose which long-term memory should forget. Afterward, add the cell state with the new input from Input Gate to update long-term memory.\n",
    "\n",
    "3. Output Gate : We'll combine hidden state with input again (but on different weight like other gates) an put them into sigmoid function. Afterward, multiply them with tanh function of cell state. The result will be both output and the next hidden state. Cell state that we got from the forget gate will be the next cell state.\n",
    "\n",
    "** It might be easier to imagine that sigmoid function parts are acting like filters, and tanh functions are the real input that we would like to filter.\n",
    "\n",
    "** This is the link that can illustrate LSTM very well. \n",
    "\n",
    "https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> LSTM - Implementation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_size = 5\n",
    "batch_size = 1\n",
    "layer_size = 1\n",
    "hidden_size = 10\n",
    "\n",
    "lstm = nn.LSTM(input_size,hidden_size,layer_size,batch_first=True) # bidirectional = True if you want to!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 3 # Like the number of word that we want to produce\n",
    "num_direction = 1 # If bidirectional=True, num_direction should be 2\n",
    "test_input = torch.randn(batch_size,seq_len,input_size)\n",
    "hidden_state = torch.randn(num_direction*layer_size,batch_size,hidden_size)\n",
    "cell_state = torch.randn(num_direction*layer_size,batch_size,hidden_size)\n",
    "hidden = (hidden_state,cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape : torch.Size([1, 3, 10])\n",
      "tensor([[[ 0.2540, -0.3610,  0.2417, -0.2368,  0.1010, -0.2607,  0.2703,\n",
      "           0.4050, -0.0911, -0.1008],\n",
      "         [ 0.0984, -0.1230,  0.1072, -0.1145,  0.0904, -0.2360,  0.2496,\n",
      "           0.2202,  0.1220,  0.0013],\n",
      "         [ 0.1500,  0.0331,  0.0622, -0.0075,  0.0150, -0.1451,  0.1656,\n",
      "           0.1173,  0.2677, -0.0663]]], grad_fn=<TransposeBackward0>)\n",
      "Hidden shape : (tensor([[[ 1.0455, -0.5382,  0.1759,  1.3397,  0.1370, -0.6164, -0.4379,\n",
      "          -0.4463,  1.1763,  0.1476]]]), tensor([[[ 0.5939, -1.3600,  1.0083, -0.9723, -0.0402, -1.9636, -0.3246,\n",
      "           1.2137,  0.1233, -0.0853]]]))\n"
     ]
    }
   ],
   "source": [
    "out, hid = lstm(test_input,hidden)\n",
    "print(\"Output shape : \" + str(out.shape)) # This is how to produce all output\n",
    "print(out)\n",
    "print(\"Hidden shape : \" + str(hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1500,  0.0331,  0.0622, -0.0075,  0.0150, -0.1451,  0.1656,  0.1173,\n",
      "         0.2677, -0.0663], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "one_out = out.squeeze()[-1,:]\n",
    "print(one_out) # This how we get the final output "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
