{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Text Representation </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>One Hot Vector</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have to deal with text processing, we cannot use word directly to calculate or evaluate on the model. So the first thing we have to do is to encode the text to some numeric representation. The most popular representation for texts is \"vector\".\n",
    "\n",
    "Before we go any further, I would like to present you guys for the easiest way to represent the text as a vector, that is \"one-hot vector.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['animal'],\n",
       "       ['dolphin']], dtype='<U7')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "vocab = np.array(['animal','bird','cow','dolphin'])\n",
    "onehot_encoder = OneHotEncoder(categories='auto')\n",
    "one_hot = onehot_encoder.fit_transform(vocab.reshape(-1,1))\n",
    "print(one_hot.toarray())\n",
    "test_doc = np.array(['animal','dolphin'])\n",
    "test_onehot = onehot_encoder.transform(test_doc.reshape(-1,1)).toarray()\n",
    "print(test_onehot)\n",
    "onehot_encoder.inverse_transform(test_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These one-hot vector can have a summation to combine words in a sentence to one vector. That will be important in CBOW algorithm in Word2Vec. (One hot vector is also used in skip-gram algorithm in Word2Vec ). We'll talk about this at the bottom of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> TF-IDF Vectorizer </h2>\n",
    "When we come up with something more complicated, the first thing we should consider how the word can effect the context is \"frequency\". So the word \"frequency\" will play an essential role on this type of vector. Let's have a close look on each technical term for this vector.\n",
    "<h3> TF : Term Frequency </h3>\n",
    "TF value can be calculate directly like the meaning of this term, which is the frequencey of the specific word in the specific document. (I'll show an example in the code below.) \n",
    "For now just now the formula is below.**\n",
    "\n",
    "$$ TF(w,d) = \\frac{\\text{The number of word w in document d}}{\\text{Total number of word in document d}}$$\n",
    "\n",
    "<h3>IDF : Inverse Document Frequency</h3>\n",
    "IDF value can call in another word as specificity. The more idf value, The more uniqueness. This value also help to remove stopword as well. Same as TF value, there are various way to calculate IDF value**. So this is one of them.\n",
    "\n",
    "$$ IDF(w) = \\log(\\frac{\\text{Total number of documents}}{\\text{The number of documents occurs word w}}) $$\n",
    "\n",
    "**Both TF and IDF have various way to calculate, see more at https://en.wikipedia.org/wiki/Tf%E2%80%93idf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we don't have to do all of that stuff by ourselves. sklearn also has a function TfidfVectorizer that you can use it easily. \n",
    "This is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names : \n",
      "['are', 'finally', 'first', 'hello', 'hi', 'in', 'is', 'last', 'second', 'sentence', 'the', 'there', 'third', 'this', 'up', 'we', 'what', 'yo']\n",
      "Vector for the first document : \n",
      "[[0.         0.         0.54558875 0.54558875 0.         0.\n",
      "  0.34824223 0.         0.         0.28471084 0.28471084 0.\n",
      "  0.         0.34824223 0.         0.         0.         0.        ]]\n",
      "Size of Transformed vector: \n",
      "(4, 18)\n",
      "The vector that transform from a new sentence:\n",
      "  (0, 15)\t0.43003651715871155\n",
      "  (0, 13)\t0.27448673838643983\n",
      "  (0, 11)\t0.43003651715871155\n",
      "  (0, 5)\t0.43003651715871155\n",
      "  (0, 3)\t0.43003651715871155\n",
      "  (0, 0)\t0.43003651715871155\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'hello this is the first sentence',\n",
    "    'hi there this is the second sentence',\n",
    "    'what is up this is the third sentence',\n",
    "    'yo finally we are in the last sentence'\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(\"Feature Names : \")\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"Vector for the first document : \")\n",
    "print(X[0].toarray())\n",
    "print(\"Size of Transformed vector: \")\n",
    "print(X.shape)\n",
    "print(\"The vector that transform from a new sentence:\")\n",
    "print(vectorizer.transform(['hello there we are in this together']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Interesting Fact !! </h4>\n",
    "\n",
    " I've just realized that we can do some preprocessing before TfidfVectorizer tokenize words (such as cleaning text) by assign \"preprocessor\" attribute when we construct TfidfVectorizer.\n",
    " You can study more on this link : \n",
    " \n",
    " https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
