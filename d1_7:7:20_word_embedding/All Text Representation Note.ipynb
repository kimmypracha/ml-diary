{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Text Representation </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>One Hot Vector</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have to deal with text processing, we cannot use word directly to calculate or evaluate on the model. So the first thing we have to do is to encode the text to some numeric representation. The most popular representation for texts is \"vector\".\n",
    "\n",
    "Before we go any further, I would like to present you guys for the easiest way to represent the text as a vector, that is \"one-hot vector.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['animal'],\n",
       "       ['dolphin']], dtype='<U7')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "vocab = np.array(['animal','bird','cow','dolphin'])\n",
    "onehot_encoder = OneHotEncoder(categories='auto')\n",
    "one_hot = onehot_encoder.fit_transform(vocab.reshape(-1,1))\n",
    "print(one_hot.toarray())\n",
    "test_doc = np.array(['animal','dolphin'])\n",
    "test_onehot = onehot_encoder.transform(test_doc.reshape(-1,1)).toarray()\n",
    "print(test_onehot)\n",
    "onehot_encoder.inverse_transform(test_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These one-hot vector can have a summation to combine words in a sentence to one vector. That will be important in CBOW algorithm in Word2Vec. (One hot vector is also used in skip-gram algorithm in Word2Vec ). We'll talk about this at the bottom of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> TF-IDF Vectorizer </h2>\n",
    "When we come up with something more complicated, the first thing we should consider how the word can effect the context is \"frequency\". So the word \"frequency\" will play an essential role on this type of vector. Let's have a close look on each technical term for this vector.\n",
    "<h3> TF : Term Frequency </h3>\n",
    "TF value can be calculate directly like the meaning of this term, which is the frequencey of the specific word in the specific document. (I'll show an example in the code below.) \n",
    "For now just now the formula is below.**\n",
    "\n",
    "$$ TF(w,d) = \\frac{\\text{The number of word w in document d}}{\\text{Total number of word in document d}}$$\n",
    "\n",
    "<h3>IDF : Inverse Document Frequency</h3>\n",
    "IDF value can call in another word as specificity. The more idf value, The more uniqueness. This value also help to remove stopword as well. Same as TF value, there are various way to calculate IDF value**. So this is one of them.\n",
    "\n",
    "$$ IDF(w) = \\log(\\frac{\\text{Total number of documents}}{\\text{The number of documents occurs word w}}) $$\n",
    "\n",
    "**Both TF and IDF have various way to calculate, see more at https://en.wikipedia.org/wiki/Tf%E2%80%93idf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we don't have to do all of that stuff by ourselves. sklearn also has a function TfidfVectorizer that you can use it easily. \n",
    "This is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names : \n",
      "['are', 'finally', 'first', 'hello', 'hi', 'in', 'is', 'last', 'second', 'sentence', 'the', 'there', 'third', 'this', 'up', 'we', 'what', 'yo']\n",
      "Vector for the first document : \n",
      "[[0.         0.         0.54558875 0.54558875 0.         0.\n",
      "  0.34824223 0.         0.         0.28471084 0.28471084 0.\n",
      "  0.         0.34824223 0.         0.         0.         0.        ]]\n",
      "Size of Transformed vector: \n",
      "(4, 18)\n",
      "The vector that transform from a new sentence:\n",
      "  (0, 15)\t0.43003651715871155\n",
      "  (0, 13)\t0.27448673838643983\n",
      "  (0, 11)\t0.43003651715871155\n",
      "  (0, 5)\t0.43003651715871155\n",
      "  (0, 3)\t0.43003651715871155\n",
      "  (0, 0)\t0.43003651715871155\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'hello this is the first sentence',\n",
    "    'hi there this is the second sentence',\n",
    "    'what is up this is the third sentence',\n",
    "    'yo finally we are in the last sentence'\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(\"Feature Names : \")\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"Vector for the first document : \")\n",
    "print(X[0].toarray())\n",
    "print(\"Size of Transformed vector: \")\n",
    "print(X.shape)\n",
    "print(\"The vector that transform from a new sentence:\")\n",
    "print(vectorizer.transform(['hello there we are in this together']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Interesting Fact !! </h4>\n",
    "\n",
    " I've just realized that we can do some preprocessing before TfidfVectorizer tokenize words (such as cleaning text) by assign \"preprocessor\" attribute when we construct TfidfVectorizer.\n",
    " You can study more on this link : \n",
    " \n",
    " https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Word2Vec</h2>\n",
    "\n",
    "Now, we are going to use something more popular for the word embedding, Word2Vec. Word2Vec use corpus to train with neural network. Word2Vec has 2 algorithm to train the vector, Skip-gram and CBOW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Skip-gram</h3>\n",
    "\n",
    "Skip-gram is the algorithm that use the word to predict the context. This skip-gram model can learn very well even with the small data, and it can handle rare words very well but it take time longer than CBOW to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>CBOW : Continuous Bag of Words</h3>\n",
    "\n",
    "CBOW is the algorithm that use contexts to predict the word. This model can train faster than Skip-gram. The accuracy is slightly better than skipgram when dealing with frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU cores : 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('have', 0.9511440992355347),\n",
       " ('am', 0.9339777231216431),\n",
       " ('help', 0.6450687646865845),\n",
       " ('lot', 0.4974794089794159),\n",
       " ('guys', 0.4706776738166809),\n",
       " ('be', 0.42309653759002686),\n",
       " ('our', 0.36935853958129883),\n",
       " ('thing', 0.33716315031051636),\n",
       " ('hi', 0.27980124950408936),\n",
       " ('invent', 0.2632341980934143)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "cores = multiprocessing.cpu_count()\n",
    "print(\"CPU cores : \" + str(cores))\n",
    "corpus = [\n",
    "    'hello everyone i am a researcher',\n",
    "    'hi guys i want to be a programmer',\n",
    "    'programmer can code like a magician',\n",
    "    'researcher can invent a new thing',\n",
    "    'both researcher and programmer help our world to grow',\n",
    "    'programmer and research have to learn a lot'\n",
    "]\n",
    "corpus = [x.split() for x in corpus]\n",
    "#This model will use skip-gram algorithm\n",
    "w2v_model = Word2Vec(min_count=1, # the minimum word frequency that would be considered\n",
    "                     window=5, # The size of windows that take all the word to consider\n",
    "                     size=5, # The dimensionality of feature vectors\n",
    "                     sample=6e-5,  # Threshold value for configuring which high-frequency word should be downsampled\n",
    "                     alpha=0.03, # The initial learning rate\n",
    "                     min_alpha=0.0007, # Learning rate will linearly drop to min_alpha\n",
    "                     negative=20, # if > 0, it will use negative sampling (how many noise should be drown)\n",
    "                     workers=cores-1, # The number of core processor (thread) to train the model\n",
    "                     sg=1)  # Just the flag for using skip-gram algorithm\n",
    "w2v_model.build_vocab(corpus,progress_per=10)\n",
    "w2v_model.train(corpus,\n",
    "                total_examples=w2v_model.corpus_count,\n",
    "                epochs=10,\n",
    "                report_delay=1)\n",
    "w2v_model.init_sims(replace=True) # Precomputing L2-Norm for wordvector and make it memory-efficient Since we no longer train anymore\n",
    "w2v_model.wv.most_similar(positive=['programmer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Word2Vec model above, if you want to change to CBOW algorithm. You just change the parameter sg=0, or just igore it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>GloVe</h2>\n",
    "This vector is invented by Standford NLP research group. Actually you don't have to implement that all by yourself, but it's also good to know how it's work. The GloVe stand for \"Global Vector\". This vector is count-based vector that store the frequency of words in the matrix VxV, that V is the number of vocabulary that we have.\n",
    "$X_{ij}$ is represent the frequency (sometime they call \"point\"**) of the word $w_i$ with context $w_j$ \n",
    "\n",
    "To reduce noise word, we use logarithm to normalize all the value in matrix X, so we can calculate word vector $w$ and $\\bar{w}$ that respect this condition.\n",
    "\n",
    "$$\n",
    "                                   w_i \\cdot \\bar{w_j} + b_i + \\bar{b_j} = \\log{X_{ij}}\n",
    "$$\n",
    "\n",
    "Which $b_i$ and $\\bar{b_j}$ are bias constant for $w_i$ and $\\bar{w_j}$ respectively.\n",
    "\n",
    "So we can calculate the cost function like this : \n",
    "\n",
    "$$\n",
    "                                    J = \\sum_{i,j} G(X_{ij})(w_i \\cdot \\bar{w_j} + b_i + \\bar{b_j} - \\log{X_{ij}})\n",
    "$$\n",
    "Which $G(X_{i,j})$ is a weighting function that \n",
    "\n",
    "$$\n",
    "                                    G(x) = \n",
    "\\begin{cases}\n",
    "      (x/x_{max})^{\\alpha} \\hspace{2.5cm}       ; x < x_{max} \\\\ \\\\\n",
    "      1 \\hspace{4cm}                          ; otherwise\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and we'll minimize the cost and adjust the parameter by using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Corpus' from 'glove' (//anaconda3/lib/python3.7/site-packages/glove/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8ac6489338c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mglove\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Corpus' from 'glove' (//anaconda3/lib/python3.7/site-packages/glove/__init__.py)"
     ]
    }
   ],
   "source": [
    "from glove import Corpus, Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting glove_python\n",
      "  Using cached https://files.pythonhosted.org/packages/3e/79/7e7e548dd9dcb741935d031117f4bed133276c2a047aadad42f1552d1771/glove_python-0.1.0.tar.gz\n",
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.7/site-packages (from glove_python) (1.16.4)\n",
      "Requirement already satisfied: scipy in /anaconda3/lib/python3.7/site-packages (from glove_python) (1.4.1)\n",
      "Building wheels for collected packages: glove-python\n",
      "  Building wheel for glove-python (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: //anaconda3/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/j1/_14trtl10y73wf576sb4l3bm4584lb/T/pip-install-n265nbqf/glove-python/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/j1/_14trtl10y73wf576sb4l3bm4584lb/T/pip-install-n265nbqf/glove-python/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /private/var/folders/j1/_14trtl10y73wf576sb4l3bm4584lb/T/pip-wheel-5hwqcszm --python-tag cp37\n",
      "       cwd: /private/var/folders/j1/_14trtl10y73wf576sb4l3bm4584lb/T/pip-install-n265nbqf/glove-python/\n",
      "  Complete output (16 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.macosx-10.7-x86_64-3.7\n",
      "  creating build/lib.macosx-10.7-x86_64-3.7/glove\n",
      "  copying glove/__init__.py -> build/lib.macosx-10.7-x86_64-3.7/glove\n",
      "  copying glove/glove.py -> build/lib.macosx-10.7-x86_64-3.7/glove\n",
      "  copying glove/corpus.py -> build/lib.macosx-10.7-x86_64-3.7/glove\n",
      "  running build_ext\n",
      "  building 'glove.glove_cython' extension\n",
      "  creating build/temp.macosx-10.7-x86_64-3.7\n",
      "  creating build/temp.macosx-10.7-x86_64-3.7/glove\n",
      "  gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I//anaconda3/include -arch x86_64 -I//anaconda3/include -arch x86_64 -I//anaconda3/include/python3.7m -c glove/glove_cython.c -o build/temp.macosx-10.7-x86_64-3.7/glove/glove_cython.o -fopenmp -ffast-math -march=native\n",
      "  clang: error: unsupported option '-fopenmp'\n",
      "  error: command 'gcc' failed with exit status 1\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for glove-python\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for glove-python\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: //anaconda3/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/j1/_14trtl10y73wf576sb4l3bm4584lb/T/pip-install-n265nbqf/glove-python/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/j1/_14trtl10y73wf576sb4l3bm4584lb/T/pip-install-n265nbqf/glove-python/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' clean --all\n",
      "       cwd: /private/var/folders/j1/_14trtl10y73wf576sb4l3bm4584lb/T/pip-install-n265nbqf/glove-python\n",
      "  Complete output (6 lines):\n",
      "  usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n",
      "     or: setup.py --help [cmd1 cmd2 ...]\n",
      "     or: setup.py --help-commands\n",
      "     or: setup.py cmd --help\n",
      "  \n",
      "  error: option --all not recognized\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed cleaning build dir for glove-python\u001b[0m\n",
      "Failed to build glove-python\n",
      "Installing collected packages: glove-python\n",
      "  Running setup.py install for glove-python ... \u001b[?25lerror\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: //anaconda3/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/j1/_14trtl10y73wf576sb4l3bm4584lb/T/pip-install-n265nbqf/glove-python/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/j1/_14trtl10y73wf576sb4l3bm4584lb/T/pip-install-n265nbqf/glove-python/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/j1/_14trtl10y73wf576sb4l3bm4584lb/T/pip-record-ef6t7cwc/install-record.txt --single-version-externally-managed --compile\n",
      "         cwd: /private/var/folders/j1/_14trtl10y73wf576sb4l3bm4584lb/T/pip-install-n265nbqf/glove-python/\n",
      "    Complete output (8 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    running build_ext\n",
      "    building 'glove.glove_cython' extension\n",
      "    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I//anaconda3/include -arch x86_64 -I//anaconda3/include -arch x86_64 -I//anaconda3/include/python3.7m -c glove/glove_cython.c -o build/temp.macosx-10.7-x86_64-3.7/glove/glove_cython.o -fopenmp -ffast-math -march=native\n",
      "    clang: error: unsupported option '-fopenmp'\n",
      "    error: command 'gcc' failed with exit status 1\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: //anaconda3/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/j1/_14trtl10y73wf576sb4l3bm4584lb/T/pip-install-n265nbqf/glove-python/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/j1/_14trtl10y73wf576sb4l3bm4584lb/T/pip-install-n265nbqf/glove-python/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/j1/_14trtl10y73wf576sb4l3bm4584lb/T/pip-record-ef6t7cwc/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install glove_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
