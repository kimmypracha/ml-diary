{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Text Representation </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>One Hot Vector</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have to deal with text processing, we cannot use word directly to calculate or evaluate on the model. So the first thing we have to do is to encode the text to some numeric representation. The most popular representation for texts is \"vector\".\n",
    "\n",
    "Before we go any further, I would like to present you guys for the easiest way to represent the text as a vector, that is \"one-hot vector.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['animal'],\n",
       "       ['dolphin']], dtype='<U7')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "vocab = np.array(['animal','bird','cow','dolphin'])\n",
    "onehot_encoder = OneHotEncoder(categories='auto')\n",
    "one_hot = onehot_encoder.fit_transform(vocab.reshape(-1,1))\n",
    "print(one_hot.toarray())\n",
    "test_doc = np.array(['animal','dolphin'])\n",
    "test_onehot = onehot_encoder.transform(test_doc.reshape(-1,1)).toarray()\n",
    "print(test_onehot)\n",
    "onehot_encoder.inverse_transform(test_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These one-hot vector can have a summation to combine words in a sentence to one vector. That will be important in CBOW algorithm in Word2Vec. (One hot vector is also used in skip-gram algorithm in Word2Vec ). We'll talk about this at the bottom of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> TF-IDF Vectorizer </h2>\n",
    "When we come up with something more complicated, the first thing we should consider how the word can effect the context is \"frequency\". So the word \"frequency\" will play an essential role on this type of vector. Let's have a close look on each technical term for this vector.\n",
    "<h3> TF : Term Frequency </h3>\n",
    "TF value can be calculate directly like the meaning of this term, which is the frequencey of the specific word in the specific document. (I'll show an example in the code below.) \n",
    "For now just now the formula is below.**\n",
    "\n",
    "$$ TF(w,d) = \\frac{\\text{The number of word w in document d}}{\\text{Total number of word in document d}}$$\n",
    "\n",
    "<h3>IDF : Inverse Document Frequency</h3>\n",
    "IDF value can call in another word as specificity. The more idf value, The more uniqueness. This value also help to remove stopword as well. Same as TF value, there are various way to calculate IDF value**. So this is one of them.\n",
    "\n",
    "$$ IDF(w) = \\log(\\frac{\\text{Total number of documents}}{\\text{The number of documents occurs word w}}) $$\n",
    "\n",
    "**Both TF and IDF have various way to calculate, see more at https://en.wikipedia.org/wiki/Tf%E2%80%93idf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we don't have to do all of that stuff by ourselves. sklearn also has a function TfidfVectorizer that you can use it easily. \n",
    "This is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names : \n",
      "['are', 'finally', 'first', 'hello', 'hi', 'in', 'is', 'last', 'second', 'sentence', 'the', 'there', 'third', 'this', 'up', 'we', 'what', 'yo']\n",
      "Vector for the first document : \n",
      "[[0.         0.         0.54558875 0.54558875 0.         0.\n",
      "  0.34824223 0.         0.         0.28471084 0.28471084 0.\n",
      "  0.         0.34824223 0.         0.         0.         0.        ]]\n",
      "Size of Transformed vector: \n",
      "(4, 18)\n",
      "The vector that transform from a new sentence:\n",
      "  (0, 15)\t0.43003651715871155\n",
      "  (0, 13)\t0.27448673838643983\n",
      "  (0, 11)\t0.43003651715871155\n",
      "  (0, 5)\t0.43003651715871155\n",
      "  (0, 3)\t0.43003651715871155\n",
      "  (0, 0)\t0.43003651715871155\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'hello this is the first sentence',\n",
    "    'hi there this is the second sentence',\n",
    "    'what is up this is the third sentence',\n",
    "    'yo finally we are in the last sentence'\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(\"Feature Names : \")\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"Vector for the first document : \")\n",
    "print(X[0].toarray())\n",
    "print(\"Size of Transformed vector: \")\n",
    "print(X.shape)\n",
    "print(\"The vector that transform from a new sentence:\")\n",
    "print(vectorizer.transform(['hello there we are in this together']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Interesting Fact !! </h4>\n",
    "\n",
    " I've just realized that we can do some preprocessing before TfidfVectorizer tokenize words (such as cleaning text) by assign \"preprocessor\" attribute when we construct TfidfVectorizer.\n",
    " You can study more on this link : \n",
    " \n",
    " https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Word2Vec</h2>\n",
    "\n",
    "Now, we are going to use something more popular for the word embedding, Word2Vec. Word2Vec use corpus to train with neural network. Word2Vec has 2 algorithm to train the vector, Skip-gram and CBOW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Skip-gram</h3>\n",
    "\n",
    "Skip-gram is the algorithm that use the word to predict the context. This skip-gram model can learn very well even with the small data, and it can handle rare words very well but it take time longer than CBOW to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU cores : 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('have', 0.9511440992355347),\n",
       " ('am', 0.9339777231216431),\n",
       " ('help', 0.6450687646865845),\n",
       " ('lot', 0.4974794089794159),\n",
       " ('guys', 0.4706776738166809),\n",
       " ('be', 0.42309653759002686),\n",
       " ('our', 0.36935853958129883),\n",
       " ('thing', 0.33716315031051636),\n",
       " ('hi', 0.27980124950408936),\n",
       " ('invent', 0.2632341980934143)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "cores = multiprocessing.cpu_count()\n",
    "print(\"CPU cores : \" + str(cores))\n",
    "corpus = [\n",
    "    'hello everyone i am a researcher',\n",
    "    'hi guys i want to be a programmer',\n",
    "    'programmer can code like a magician',\n",
    "    'researcher can invent a new thing',\n",
    "    'both researcher and programmer help our world to grow',\n",
    "    'programmer and research have to learn a lot'\n",
    "]\n",
    "corpus = [x.split() for x in corpus]\n",
    "\n",
    "w2v_model = Word2Vec(min_count=1, # the minimum word frequency that would be considered\n",
    "                     window=5, # The size of windows that take all the word to consider\n",
    "                     size=5, # The dimensionality of feature vectors\n",
    "                     sample=6e-5,  # Threshold value for configuring which high-frequency word should be downsampled\n",
    "                     alpha=0.03, # The initial learning rate\n",
    "                     min_alpha=0.0007, # Learning rate will linearly drop to min_alpha\n",
    "                     negative=20, # if > 0, it will use negative sampling (how many noise should be drown)\n",
    "                     workers=cores-1, # The number of core processor (thread) to train the model\n",
    "                     sg=1)  \n",
    "w2v_model.build_vocab(corpus,progress_per=10)\n",
    "w2v_model.train(corpus,\n",
    "                total_examples=w2v_model.corpus_count,\n",
    "                epochs=10,\n",
    "                report_delay=1)\n",
    "w2v_model.init_sims(replace=True) # Precomputing L2-Norm for wordvector and make it memory-efficient Since we no longer train anymore\n",
    "w2v_model.wv.most_similar(positive=['programmer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>CBOW : Continuous Bag of Words</h3>\n",
    "\n",
    "CBOW is the algorithm that use contexts to predict the word. This model can train faster than Skip-gram. The accuracy is slightly better than skipgram when dealing with frequent words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
