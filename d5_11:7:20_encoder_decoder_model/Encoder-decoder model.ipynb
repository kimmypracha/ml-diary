{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Encoder-Decoder model </h1>\n",
    "\n",
    "Full Explanation here :\n",
    "https://medium.com/analytics-vidhya/machine-translation-encoder-decoder-model-7e4867377161#:~:text=The%20encoder%2Ddecoder%20model%20is,%2Dto%2Dsequence%20prediction%20problems.&text=The%20approach%20involves%20two%20recurrent,target%20sequence%20called%20the%20decoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the topic of this notebook, this model has two parts, encoder and decoder. This type of model is important for knowing BERT, Attention mechanism, and Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example scenario of this model is when you want to translate Thai language to English language or vise versa. Also, you can take some images and gives out some caption of that image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> The standard mechanism for encoder-decoder model</h2>\n",
    "\n",
    "First of all, this model has two parts, encoder and decoder. In the encoder part, there are some LSTM or RNN inside to take each word vector in the sequence to give the last hidden state, and cell state $h_l$ and $c_l$ respectively.  and then we will use $h_l$ and $c_l$ as an initial hidden state and cell state respectively for the decoder part, which also include LSTM or RNN inside like encoder part.\n",
    "\n",
    "At the first time we put the < start > which is a start token for the sequence of word. in the first LSTM in the decoder, so we expected the input to be the first word of the output text. Then we will use that word as an input for the next LSTM in the decoder to get the next decoded word and so on.\n",
    "\n",
    "Read more detail at : \n",
    "https://medium.com/analytics-vidhya/machine-translation-encoder-decoder-model-7e4867377161#:~:text=The%20encoder%2Ddecoder%20model%20is,%2Dto%2Dsequence%20prediction%20problems.&text=The%20approach%20involves%20two%20recurrent,target%20sequence%20called%20the%20decoder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Attention Mechanism </h1>\n",
    "\n",
    "When we have to learn something, instead of learn from the whole book, it would be better if we can focus on something that is really important. So this is why the Attention mechanism come from. Like the encoder-decoder model we described above. We will collect all the previous hidden state from the encoder part, and use the new hidden state from decoder at each point to calculate the context vector. After we got the context vector, we will concatenate this context vector with the new hidden vector. and feed it into the feedforward neural network. So we can get the decoded word.\n",
    "\n",
    "More about mathematical notation at (soft alignments): \n",
    "https://krntneja.github.io/posts/2018/attention-based-models-1\n",
    "\n",
    "More about visualization at : \n",
    "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/?fbclid=IwAR24X5IHtHbhKF61R0wN-27MMZIN-TeGfKiI879RsvFgrWift80GY932r7k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 50])\n",
      "torch.Size([100, 50])\n",
      "torch.Size([100, 50])\n",
      "torch.Size([100, 50])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "features_dim = 50\n",
    "hidden_dim = 100\n",
    "n_layers = 1\n",
    "seq_len = 4\n",
    "n_batch = 100\n",
    "test_input = torch.randn(n_batch,seq_len,features_dim)\n",
    "#Encoder-Decoder Example implementation by me.\n",
    "class encoder_decoder_model(nn.module):\n",
    "    def __init__(self):\n",
    "        super(encoder_decoder_model,self).__init__()\n",
    "        self.encoder = nn.RNN(features_dim,hidden_dim,n_layers,batch_first=True)\n",
    "        self.decoder = nn.RNN(features_dim,hidden_dim,n_layers,batch_first=True)\n",
    "    def forward(self,X):\n",
    "        out_list = []\n",
    "        h = torch.randn(n_layers,n_batch,features_dim)\n",
    "        for i in range(1,seq_len): # Ignore <start> token\n",
    "            out, h = self.encoder(X[:,i,:],h)\n",
    "        for i in range(seq_len): # Use <start> token\n",
    "            out, h = self.decoder(X,h)\n",
    "            out_list.append(out)\n",
    "        return out_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
